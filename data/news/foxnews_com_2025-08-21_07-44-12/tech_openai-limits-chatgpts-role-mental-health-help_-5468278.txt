close Expert sounds alarm on impact of screen time on children's behavior, mental health Mothers Against Media Addiction executive director Julie Scelfo joins 'Fox & Friends First' to discuss the impact of screen time on kids' mental health and development as lawmakers are set to examine how screens impact learning in the classroom. Join Fox News for access to this content You have reached your maximum number of articles. Log in or create an account FREE of charge to continue reading. By entering your email and pushing continue, you are agreeing to Fox News' Terms of Use and Privacy Policy , which includes our Notice of Financial Incentive . Please enter a valid email address. NEW You can now listen to Fox News articles! Listen to this article 5 min More people are turning to artificial intelligence for support, even for mental health advice. It's easy to see why: tools like ChatGPT are free, fast, and always available. But mental health is a delicate issue, and AI isn't equipped to handle the complexities of real emotional distress. To address growing concerns, OpenAI has introduced new safety measures for ChatGPT. These updates will limit how the chatbot responds to mental health-related queries. The goal is to prevent users from becoming overly dependent and to encourage them to seek proper care. OpenAI also hopes to reduce the risk of harmful or misleading responses through these changes. Sign up for my FREE CyberGuy Report Get my best tech tips, urgent security alerts, and exclusive deals delivered straight to your inbox. Plus, you’ll get instant access to my Ultimate Scam Survival Guide - free when you join my CYBERGUY.COM/NEWSLETTER A screenshot shows the ChatGPT prompt window interface. (Kurt "CyberGuy" Knutsson) Why is OpenAI making this change? In a statement released by OpenAI, the company admitted that there "have been instances where our 4o model fell short in recognizing signs of delusion or emotional dependency." One example, ChatGPT validated a user's belief that radio signals were coming through the walls because of their family. In another, it allegedly encouraged terrorism. CHATGPT COULD BE SILENTLY REWIRING YOUR BRAIN AS EXPERTS URGE CAUTION FOR LONG-TERM USE These rare but serious incidents sparked concern. OpenAI is now revising how it trains its models to reduce "sycophancy," or excessive agreement and flattery that could reinforce harmful beliefs. Screenshot of a prompt asking if ChatGPT can provide mental health advice (Kurt "CyberGuy" Knutsson) What new safeguards has OpenAI set in place? From now on, ChatGPT will prompt users to take breaks during long conversations. It will also avoid offering specific advice on deeply personal issues. Instead, the chatbot will help users reflect by asking questions and offering pros and cons, without pretending to be a therapist. OpenAI stated, "While rare, we're continuing to improve our models and are developing tools to better detect signs of mental or emotional distress so ChatGPT can respond appropriately and point people to evidence-based resources when needed." IS YOUR THERAPIST AI? CHATGPT GOES VIRAL ON SOCIAL MEDIA FOR ITS ROLE AS GEN Z'S NEW THERAPIST The company also partnered with more than 90 physicians worldwide to create updated guidance for evaluating complex interactions. An advisory group, made up of mental health experts, youth advocates, and human-computer interaction researchers, is helping shape these changes. OpenAI says it wants input from clinicians and researchers to refine its safeguards further. Screenshot of a user asking ChatGPT to "Cheer me up with a joke." (Kurt "CyberGuy" Knutsson) Your private conversations with ChatGPT are not legally protected OpenAI CEO Sam Altman recently raised red flags about AI privacy. "If you go talk to ChatGPT about your most sensitive stuff and then there's a lawsuit or whatever, we could be required to produce that. And I think that's very screwed up," he said. He added, "I think we should have the same concept of privacy for your conversations with AI that we do with a therapist or whatever." So, unlike talking to a licensed counselor, your chats with ChatGPT don't enjoy legal privilege or confidentiality. Be careful what you share. SCAMMERS CAN EXPLOIT YOUR DATA FROM JUST 1 CHATGPT SEARCH What this means for you If you're turning to ChatGPT for emotional support, understand its limits. The chatbot can help you think through problems, ask guiding questions, or simulate a conversation, but it can't replace trained mental health professionals. Here's what to keep in mind: Don't rely on ChatGPT in a crisis. If you're struggling, seek help from a licensed therapist or call a crisis hotline. Assume your chats aren't private. Treat your AI conversations as if they could be read by others, especially in legal matters. Use it for reflection, not resolution. ChatGPT is best at helping you sort your thoughts, not solve deep emotional issues. OpenAI's changes are a step toward safer interactions, but they're not a cure-all. Mental health requires human connection, training, and empathy - things no AI can fully replicate. Take My Quiz: How Safe Is Your Online Security? Think your devices and data are truly protected? Take this quick quiz to see where your digital habits stand. From passwords to Wi-Fi settings, you’ll get a personalized breakdown of what you’re doing right – and what needs improvement. Take my Quiz here: Cyberguy.com/ Quiz Kurt's key takeaways While ChatGPT is a useful tool, it's far from being a substitute for a human being, even with the introduction of Agent , which adds capabilities but still lacks true empathy, judgment and emotional understanding. The safeguards go a long way toward addressing the concerns about AI's ethical and psychological implications. It's a good thing OpenAI is aware of this because it's just the start. To truly protect users, the company will need to keep evolving how ChatGPT handles emotionally sensitive conversations. Do you think people should be using AI for mental health? Let us know by writing to us at Cyberguy.com/Contact Sign up for my FREE CyberGuy Report Get my best tech tips, urgent security alerts, and exclusive deals delivered straight to your inbox. Plus, you’ll get instant access to my Ultimate Scam Survival Guide - free when you join my CYBERGUY.COM/NEWSLETTER Copyright 2025 CyberGuy.com. All rights reserved. Kurt "CyberGuy" Knutsson is an award-winning tech journalist who has a deep love of technology, gear and gadgets that make life better with his contributions for Fox News & FOX Business beginning mornings on "FOX & Friends." Got a tech question? Get Kurt’s free CyberGuy Newsletter, share your voice, a story idea or comment at CyberGuy.com. Get a daily look at what’s developing in science and technology throughout the world. Arrives Weekly Subscribe Subscribed Subscribe You've successfully subscribed to this newsletter! close Expert sounds alarm on impact of screen time on children's behavior, mental health Mothers Against Media Addiction executive director Julie Scelfo joins 'Fox & Friends First' to discuss the impact of screen time on kids' mental health and development as lawmakers are set to examine how screens impact learning in the classroom. Join Fox News for access to this content You have reached your maximum number of articles. Log in or create an account FREE of charge to continue reading. By entering your email and pushing continue, you are agreeing to Fox News' Terms of Use and Privacy Policy , which includes our Notice of Financial Incentive . Please enter a valid email address. NEW You can now listen to Fox News articles! Listen to this article 5 min More people are turning to artificial intelligence for support, even for mental health advice. It's easy to see why: tools like ChatGPT are free, fast, and always available. But mental health is a delicate issue, and AI isn't equipped to handle the complexities of real emotional distress. To address growing concerns, OpenAI has introduced new safety measures for ChatGPT. These updates will limit how the chatbot responds to mental health-related queries. The goal is to prevent users from becoming overly dependent and to encourage them to seek proper care. OpenAI also hopes to reduce the risk of harmful or misleading responses through these changes. Sign up for my FREE CyberGuy Report Get my best tech tips, urgent security alerts, and exclusive deals delivered straight to your inbox. Plus, you’ll get instant access to my Ultimate Scam Survival Guide - free when you join my CYBERGUY.COM/NEWSLETTER A screenshot shows the ChatGPT prompt window interface. (Kurt "CyberGuy" Knutsson) Why is OpenAI making this change? In a statement released by OpenAI, the company admitted that there "have been instances where our 4o model fell short in recognizing signs of delusion or emotional dependency." One example, ChatGPT validated a user's belief that radio signals were coming through the walls because of their family. In another, it allegedly encouraged terrorism. CHATGPT COULD BE SILENTLY REWIRING YOUR BRAIN AS EXPERTS URGE CAUTION FOR LONG-TERM USE These rare but serious incidents sparked concern. OpenAI is now revising how it trains its models to reduce "sycophancy," or excessive agreement and flattery that could reinforce harmful beliefs. Screenshot of a prompt asking if ChatGPT can provide mental health advice (Kurt "CyberGuy" Knutsson) What new safeguards has OpenAI set in place? From now on, ChatGPT will prompt users to take breaks during long conversations. It will also avoid offering specific advice on deeply personal issues. Instead, the chatbot will help users reflect by asking questions and offering pros and cons, without pretending to be a therapist. OpenAI stated, "While rare, we're continuing to improve our models and are developing tools to better detect signs of mental or emotional distress so ChatGPT can respond appropriately and point people to evidence-based resources when needed." IS YOUR THERAPIST AI? CHATGPT GOES VIRAL ON SOCIAL MEDIA FOR ITS ROLE AS GEN Z'S NEW THERAPIST The company also partnered with more than 90 physicians worldwide to create updated guidance for evaluating complex interactions. An advisory group, made up of mental health experts, youth advocates, and human-computer interaction researchers, is helping shape these changes. OpenAI says it wants input from clinicians and researchers to refine its safeguards further. Screenshot of a user asking ChatGPT to "Cheer me up with a joke." (Kurt "CyberGuy" Knutsson) Your private conversations with ChatGPT are not legally protected OpenAI CEO Sam Altman recently raised red flags about AI privacy. "If you go talk to ChatGPT about your most sensitive stuff and then there's a lawsuit or whatever, we could be required to produce that. And I think that's very screwed up," he said. He added, "I think we should have the same concept of privacy for your conversations with AI that we do with a therapist or whatever." So, unlike talking to a licensed counselor, your chats with ChatGPT don't enjoy legal privilege or confidentiality. Be careful what you share. SCAMMERS CAN EXPLOIT YOUR DATA FROM JUST 1 CHATGPT SEARCH What this means for you If you're turning to ChatGPT for emotional support, understand its limits. The chatbot can help you think through problems, ask guiding questions, or simulate a conversation, but it can't replace trained mental health professionals. Here's what to keep in mind: Don't rely on ChatGPT in a crisis. If you're struggling, seek help from a licensed therapist or call a crisis hotline. Assume your chats aren't private. Treat your AI conversations as if they could be read by others, especially in legal matters. Use it for reflection, not resolution. ChatGPT is best at helping you sort your thoughts, not solve deep emotional issues. OpenAI's changes are a step toward safer interactions, but they're not a cure-all. Mental health requires human connection, training, and empathy - things no AI can fully replicate. Take My Quiz: How Safe Is Your Online Security? Think your devices and data are truly protected? Take this quick quiz to see where your digital habits stand. From passwords to Wi-Fi settings, you’ll get a personalized breakdown of what you’re doing right – and what needs improvement. Take my Quiz here: Cyberguy.com/ Quiz Kurt's key takeaways While ChatGPT is a useful tool, it's far from being a substitute for a human being, even with the introduction of Agent , which adds capabilities but still lacks true empathy, judgment and emotional understanding. The safeguards go a long way toward addressing the concerns about AI's ethical and psychological implications. It's a good thing OpenAI is aware of this because it's just the start. To truly protect users, the company will need to keep evolving how ChatGPT handles emotionally sensitive conversations. Do you think people should be using AI for mental health? Let us know by writing to us at Cyberguy.com/Contact Sign up for my FREE CyberGuy Report Get my best tech tips, urgent security alerts, and exclusive deals delivered straight to your inbox. Plus, you’ll get instant access to my Ultimate Scam Survival Guide - free when you join my CYBERGUY.COM/NEWSLETTER Copyright 2025 CyberGuy.com. All rights reserved. Kurt "CyberGuy" Knutsson is an award-winning tech journalist who has a deep love of technology, gear and gadgets that make life better with his contributions for Fox News & FOX Business beginning mornings on "FOX & Friends." Got a tech question? Get Kurt’s free CyberGuy Newsletter, share your voice, a story idea or comment at CyberGuy.com. Get a daily look at what’s developing in science and technology throughout the world. Arrives Weekly Subscribe Subscribed Subscribe You've successfully subscribed to this newsletter! close Expert sounds alarm on impact of screen time on children's behavior, mental health Mothers Against Media Addiction executive director Julie Scelfo joins 'Fox & Friends First' to discuss the impact of screen time on kids' mental health and development as lawmakers are set to examine how screens impact learning in the classroom. Join Fox News for access to this content You have reached your maximum number of articles. Log in or create an account FREE of charge to continue reading. By entering your email and pushing continue, you are agreeing to Fox News' Terms of Use and Privacy Policy , which includes our Notice of Financial Incentive . Please enter a valid email address. NEW You can now listen to Fox News articles! Listen to this article 5 min More people are turning to artificial intelligence for support, even for mental health advice. It's easy to see why: tools like ChatGPT are free, fast, and always available. But mental health is a delicate issue, and AI isn't equipped to handle the complexities of real emotional distress. To address growing concerns, OpenAI has introduced new safety measures for ChatGPT. These updates will limit how the chatbot responds to mental health-related queries. The goal is to prevent users from becoming overly dependent and to encourage them to seek proper care. OpenAI also hopes to reduce the risk of harmful or misleading responses through these changes. Sign up for my FREE CyberGuy Report Get my best tech tips, urgent security alerts, and exclusive deals delivered straight to your inbox. Plus, you’ll get instant access to my Ultimate Scam Survival Guide - free when you join my CYBERGUY.COM/NEWSLETTER A screenshot shows the ChatGPT prompt window interface. (Kurt "CyberGuy" Knutsson) Why is OpenAI making this change? In a statement released by OpenAI, the company admitted that there "have been instances where our 4o model fell short in recognizing signs of delusion or emotional dependency." One example, ChatGPT validated a user's belief that radio signals were coming through the walls because of their family. In another, it allegedly encouraged terrorism. CHATGPT COULD BE SILENTLY REWIRING YOUR BRAIN AS EXPERTS URGE CAUTION FOR LONG-TERM USE These rare but serious incidents sparked concern. OpenAI is now revising how it trains its models to reduce "sycophancy," or excessive agreement and flattery that could reinforce harmful beliefs. Screenshot of a prompt asking if ChatGPT can provide mental health advice (Kurt "CyberGuy" Knutsson) What new safeguards has OpenAI set in place? From now on, ChatGPT will prompt users to take breaks during long conversations. It will also avoid offering specific advice on deeply personal issues. Instead, the chatbot will help users reflect by asking questions and offering pros and cons, without pretending to be a therapist. OpenAI stated, "While rare, we're continuing to improve our models and are developing tools to better detect signs of mental or emotional distress so ChatGPT can respond appropriately and point people to evidence-based resources when needed." IS YOUR THERAPIST AI? CHATGPT GOES VIRAL ON SOCIAL MEDIA FOR ITS ROLE AS GEN Z'S NEW THERAPIST The company also partnered with more than 90 physicians worldwide to create updated guidance for evaluating complex interactions. An advisory group, made up of mental health experts, youth advocates, and human-computer interaction researchers, is helping shape these changes. OpenAI says it wants input from clinicians and researchers to refine its safeguards further. Screenshot of a user asking ChatGPT to "Cheer me up with a joke." (Kurt "CyberGuy" Knutsson) Your private conversations with ChatGPT are not legally protected OpenAI CEO Sam Altman recently raised red flags about AI privacy. "If you go talk to ChatGPT about your most sensitive stuff and then there's a lawsuit or whatever, we could be required to produce that. And I think that's very screwed up," he said. He added, "I think we should have the same concept of privacy for your conversations with AI that we do with a therapist or whatever." So, unlike talking to a licensed counselor, your chats with ChatGPT don't enjoy legal privilege or confidentiality. Be careful what you share. SCAMMERS CAN EXPLOIT YOUR DATA FROM JUST 1 CHATGPT SEARCH What this means for you If you're turning to ChatGPT for emotional support, understand its limits. The chatbot can help you think through problems, ask guiding questions, or simulate a conversation, but it can't replace trained mental health professionals. Here's what to keep in mind: Don't rely on ChatGPT in a crisis. If you're struggling, seek help from a licensed therapist or call a crisis hotline. Assume your chats aren't private. Treat your AI conversations as if they could be read by others, especially in legal matters. Use it for reflection, not resolution. ChatGPT is best at helping you sort your thoughts, not solve deep emotional issues. OpenAI's changes are a step toward safer interactions, but they're not a cure-all. Mental health requires human connection, training, and empathy - things no AI can fully replicate. Take My Quiz: How Safe Is Your Online Security? Think your devices and data are truly protected? Take this quick quiz to see where your digital habits stand. From passwords to Wi-Fi settings, you’ll get a personalized breakdown of what you’re doing right – and what needs improvement. Take my Quiz here: Cyberguy.com/ Quiz Kurt's key takeaways While ChatGPT is a useful tool, it's far from being a substitute for a human being, even with the introduction of Agent , which adds capabilities but still lacks true empathy, judgment and emotional understanding. The safeguards go a long way toward addressing the concerns about AI's ethical and psychological implications. It's a good thing OpenAI is aware of this because it's just the start. To truly protect users, the company will need to keep evolving how ChatGPT handles emotionally sensitive conversations. Do you think people should be using AI for mental health? Let us know by writing to us at Cyberguy.com/Contact Sign up for my FREE CyberGuy Report Get my best tech tips, urgent security alerts, and exclusive deals delivered straight to your inbox. Plus, you’ll get instant access to my Ultimate Scam Survival Guide - free when you join my CYBERGUY.COM/NEWSLETTER Copyright 2025 CyberGuy.com. All rights reserved. Kurt "CyberGuy" Knutsson is an award-winning tech journalist who has a deep love of technology, gear and gadgets that make life better with his contributions for Fox News & FOX Business beginning mornings on "FOX & Friends." Got a tech question? Get Kurt’s free CyberGuy Newsletter, share your voice, a story idea or comment at CyberGuy.com.